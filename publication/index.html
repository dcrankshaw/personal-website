<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37.1" />
  <meta name="author" content="Dan Crankshaw">

  
  
  
  
  <meta name="description" content="PhD Candidate">

  
  <link rel="alternate" hreflang="en-us" href="/publication/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Dan Crankshaw">
  <link rel="feed" href="/publication/index.xml" type="application/rss+xml" title="Dan Crankshaw">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/publication/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Dan Crankshaw">
  <meta property="og:url" content="/publication/">
  <meta property="og:title" content="Publications | Dan Crankshaw">
  <meta property="og:description" content="PhD Candidate">
  <meta property="og:locale" content="en-us">
  
  <meta property="og:updated_time" content="2017-01-01T00:00:00&#43;00:00">
  

  

  <title>Publications | Dan Crankshaw</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Dan Crankshaw</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>





<div class="container">
  <div class="row">
    <div class="col-md-12">
      <h1>Publications</h1>

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <p>
        Filter by type:

        <select class="pub-filters pubtype-select" data-filter-group="pubtype">
          <option value="*">All</option>
          
        </select>

        <select class="pub-filters" data-filter-group="year">
          <option value="*">All</option>
          
          
          
          <option value=".year-2018">
            2018
          </option>
          
          <option value=".year-2017">
            2017
          </option>
          
          <option value=".year-2015">
            2015
          </option>
          
          <option value=".year-2014">
            2014
          </option>
          
          <option value=".year-2013">
            2013
          </option>
          
          
        </select>
      </p>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-3 year-2018">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/inferline-pub/" itemprop="url">InferLine: ML Inference Pipeline Composition</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    The dominant cost in production machine learning workloads is not training individual models but serving predictions from increasingly complex prediction pipelines spanning multiple models, machine learning frameworks, and parallel hardware accelerators. Due to the complex interaction between model configurations and parallel hardware, prediction pipelines are challenging to provision and costly to execute when serving interactive latency-sensitive applications. This challenge is exacerbated by the unpredictable dynamics of bursty workloads. In this paper we introduce InferLine, a system which efficiently provisions and executes ML inference pipelines subject to end-to-end latency constraints by proactively optimizing and reactively controlling per-model configuration in a fine-grained fashion. Unpredictable changes in the serving workload are dynamically and cost-optimally accommodated with minimal service level degradation. InferLine introduces (1) automated model profiling and pipeline lineage extraction, (2) a fine-grain, cost-minimizing pipeline configuration planner, and (3) a fine-grain reactive controller. InferLine is able to configure and deploy prediction pipelines across a wide range of workload patterns and latency goals. It outperforms coarse-grained configuration alternatives by up 7.6x in cost while achieving up to 32x lower SLO miss rate on real workloads and generalizes across state-of-the-art model serving frameworks.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, Alexey Tumanov
    
  </div>

  <div class="pub-publication">
    2018</div>

  <div class="pub-links">
    



<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1812.01776" target="_blank" rel="noopener">
  Preprint
</a>
















  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-0 year-2018">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/model-serving-rfp-acm-queue/" itemprop="url">Research for Practice: Prediction-Serving Systems </a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Joseph E. Gonzalez
    
  </div>

  <div class="pub-publication">
    In <em>ACM Queue</em>,2018</div>

  <div class="pub-links">
    

















<a class="btn btn-primary btn-outline btn-xs" href="https://queue.acm.org/detail.cfm?id=3210557" target="_blank" rel="noopener">
  URL
</a>


  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-3 year-2017">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/cascades-pub/" itemprop="url">IDK Cascades: Fast Deep Learning by Learning not to Overthink</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    Advances in deep learning have led to substantial increases in prediction accuracy but have been accompanied by increases in the cost of rendering predictions. We conjecture that for a majority of real-world inputs, the recent advances in deep learning have created models that effectively <em>over-think</em> on simple inputs. In this paper we revisit the question of how to effectively build model cascades to reduce prediction costs. While classic cascade techniques primarily leverage class asymmetry to reduce cost, we extend this approach to arbitrary multi-class prediction tasks. We introduce the <em>I Don&rsquo;t Know</em> (IDK) prediction cascades framework, a general framework for composing a set of pre-trained models to accelerate inference without a loss in prediction accuracy. We propose two search based methods for constructing cascades as well as a new cost-aware objective within this framework. We evaluate these techniques on a range of both benchmark and real-world datasets and demonstrate that prediction cascades can reduce computation by 37%, resulting in up to 1.6x speedups in image classification tasks over state-of-the-art models without a loss in accuracy. Furthermore, on a driving motion prediction task evaluated on a large scale autonomous driving dataset, prediction cascades achieved 95% accuracy when combined with human experts, while requiring human intervention on less than 30% of the queries.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    Xin Wang, Yujia Luo, <strong>Daniel Crankshaw</strong>, Alexey Tumanov, Fisher Yu, Joseph E. Gonzalez
    
  </div>

  <div class="pub-publication">
    2017</div>

  <div class="pub-links">
    



<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/abs/1706.00885" target="_blank" rel="noopener">
  Preprint
</a>
















  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-1 year-2017">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/clipper-pub/" itemprop="url">Clipper: A Low-Latency Online Prediction Serving System</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the Tensorflow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph E. Gonzalez, Ion Stoica
    
  </div>

  <div class="pub-publication">
    In <em>NSDI</em>,
    2017</div>

  <div class="pub-links">
    




<a class="btn btn-primary btn-outline btn-xs" href="https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf" target="_blank" rel="noopener">
  PDF
</a>





<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/amplab/clipper-v0" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-primary btn-outline btn-xs" href="/project/clipper/">
  Project
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://www.usenix.org/sites/default/files/conference/protected-files/nsdi17_slides_crankshaw.pdf" target="_blank" rel="noopener">
  Slides
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw" target="_blank" rel="noopener">
  Video
</a>




  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-4 year-2015">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/learning-sys-workshop-pub/" itemprop="url">Scalable Training and Serving of Personalized Models</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Xin Wang, Joseph E. Gonzalez, Michael J. Franklin
    
  </div>

  <div class="pub-publication">
    In <em>LearningSys</em>,
    2015</div>

  <div class="pub-links">
    




<a class="btn btn-primary btn-outline btn-xs" href="http://learningsys.org/papers/LearningSys_2015_paper_27.pdf" target="_blank" rel="noopener">
  PDF
</a>















  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-1 year-2015">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/velox-pub/" itemprop="url">The Missing Piece in Complex Analytics: Low Latency, Scalable Model Management and Serving with Velox</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    To support complex data-intensive applications such as personalized recommendations, targeted advertising, and intelligent services, the data management community has focused heavily on the design of systems to support training complex models on large datasets. Unfortunately, the design of these systems largely ignores a critical component of the overall analytics process: the deployment and serving of models at scale. In this work, we present Velox, a new component of the Berkeley Data Analytics Stack. Velox is a data management system for facilitating the next steps in real-world, large-scale analytics pipelines: online model management, maintenance, and serving. Velox provides end-user applications and services with a low-latency, intuitive interface to models, transforming the raw statistical models currently trained using existing offline large-scale compute frameworks into full-blown, end-to-end data products capable of recommending products, targeting advertisements, and personalizing web content. To provide up-to-date results for these complex models, Velox also facilitates lightweight online model maintenance and selection (i.e., dynamic weighting). In this paper, we describe the challenges and architectural considerations required to achieve this functionality, including the abilities to span online and offline systems, to adaptively adjust model materialization strategies, and to exploit inherent statistical properties such as model error tolerance, all while operating at &ldquo;Big Data&rdquo; scale.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Peter Bailis, Joseph E. Gonzalez, Haoyuan Li, Zhao Zhang, Michael J. Franklin, Ali Ghodsi, Michael I. Jordan, Ion Stoica
    
  </div>

  <div class="pub-publication">
    In <em>CIDR</em>,
    2015</div>

  <div class="pub-links">
    




<a class="btn btn-primary btn-outline btn-xs" href="/files/papers/veloxms.pdf" target="_blank" rel="noopener">
  PDF
</a>





<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/amplab/velox-modelserver" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-primary btn-outline btn-xs" href="/project/velox/">
  Project
</a>








  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-1 year-2014">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/graphx-pub/" itemprop="url">GraphX: Graph Processing in a Distributed Dataflow Framework</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataflow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataflow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphX presents a familiar composable graph abstraction that is sufficient to express existing graph APIs, yet can be implemented using only a few basic dataflow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-specific optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataflow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataflow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    Joseph E. Gonzalez, Reynold S. Xin, Ankur Dave, <strong>Daniel Crankshaw</strong>, Michael J. Franklin, Ion Stoica
    
  </div>

  <div class="pub-publication">
    In <em>OSDI</em>,
    2014</div>

  <div class="pub-links">
    




<a class="btn btn-primary btn-outline btn-xs" href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/graphx-pub.bib">
  Cite
</button>





<a class="btn btn-primary btn-outline btn-xs" href="/project/graphx/">
  Project
</a>




<a class="btn btn-primary btn-outline btn-xs" href="https://www.usenix.org/sites/default/files/conference/protected-files/osdi14_slides_gonzalez.pdf" target="_blank" rel="noopener">
  Slides
</a>





  </div>

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-md-12 isotope-item pubtype-1 year-2013">
          
            <div class="pub-list-item card-simple" itemscope itemtype="http://schema.org/CreativeWork">

  
  

  <h3 class="article-title" itemprop="name">
    <a href="/publication/indra-pub/" itemprop="url">Inverted Indices for Particle Tracking in Petascale Cosmological Simulations</a>
  </h3>

  <div class="pub-abstract" itemprop="text">
    
    We describe the challenges arising from tracking dark matter particles in state of the art cosmological simulations. We are in the process of running the Indra suite of simulations, with an aggregate count of more than 35 trillion particles and 1.1PB of total raw data volume. However, it is not enough just to store the particle positions and velocities in an efficient manner &ndash; analyses also need to be able to track individual particles efficiently through the temporal history of the simulation. The required inverted indices can easily have raw sizes comparable to the original simulation. We explore various strategies on how to create an efficient index for such data, using additional insight from the physical properties of the particle motions for a greatly compressed data representation. The basic particle data are stored in a relational database in course-grained containers corresponding to leaves of a fixed depth oct-tree labeled by their Peano-Hilbert index. Within each container the individual objects are sorted by their Lagrangian identifier. Thus each particle has a multi-level address: the PH key of the container and the index of the particle within the sorted array (the slot). Given the nature of the cosmological simulations and choice of the PH-box sizes, in consecutive snapshots particles can only cross into spatially adjacent boxes. Also, the slot number of a particle in adjacent snapshots is adjusted up or down by typically a small number. As a result, a special version of delta encoding over the multi-tier address already results in a dramatic reduction of data that needs to be stored. We follow next with an efficient bit-compression, adapting to the statistical properties of the two-part addresses, achieving a final compression ratio better than a factor of 9. The final size of the full inverted index is projected to be 22.5 TB for a petabyte ensemble of simulations.
    
  </div>

  <div class="pub-authors" itemprop="author">
    
    <strong>Daniel Crankshaw</strong>, Randal Burns, Bridget Falck, Tamás Budavári, Alexander S. Szalay, Jie Wang
    
  </div>

  <div class="pub-publication">
    In <em>SSDBM</em>,
    2013</div>

  <div class="pub-links">
    




<a class="btn btn-primary btn-outline btn-xs" href="/files/papers/indra-ssdbm.pdf" target="_blank" rel="noopener">
  PDF
</a>




<button type="button" class="btn btn-primary btn-outline btn-xs js-cite-modal"
        data-filename="/files/citations/indra-pub.bib">
  Cite
</button>












  </div>

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>
<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 Daniel Crankshaw &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    

  </body>
</html>

